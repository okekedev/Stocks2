#!/usr/bin/env python3
"""
Polygon.io Complete Data Extractor
Pulls all available data with pagination and stores in SQLite
"""

import os
import sys
import time
import sqlite3
import requests
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv
import logging
from typing import Dict, List, Optional, Any

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('polygon_extraction.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class PolygonDataExtractor:
    """Extracts all available data from Polygon.io API with pagination"""
    
    def __init__(self, api_key: str, db_path: str = "polygon_complete_data.db"):
        self.api_key = api_key
        self.base_url = "https://api.polygon.io"
        self.db_path = db_path
        self.log_file = 'polygon_extraction.log'
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}'
        })
        
        # Track statistics
        self.stats = {
            'api_calls': 0,
            'total_records': 0,
            'start_time': None,
            'errors': 0
        }
        
        # Update .gitignore
        self.update_gitignore()
        
        # Initialize database
        self.init_database()
    
    def update_gitignore(self):
        """Add generated files to .gitignore"""
        gitignore_entries = [
            # Database files
            '*.db',
            '*.db-journal',
            '*.sqlite',
            '*.sqlite3',
            'polygon_complete_data.db',
            'polygon_market_data.db',
            
            # Log files
            '*.log',
            'polygon_extraction.log',
            'polygon_data_collection.log',
            
            # Backup files
            '*.db-backup',
            '*.bak',
            
            # Environment files
            '.env',
            '.env.*',
            '*.env',
            
            # Python cache
            '__pycache__/',
            '*.pyc',
            '*.pyo',
            '*.pyd',
            '.Python',
            
            # IDE files
            '.vscode/',
            '.idea/',
            '*.swp',
            '*.swo',
            '*~',
            
            # OS files
            '.DS_Store',
            'Thumbs.db',
            'desktop.ini'
        ]
        
        # Read existing .gitignore
        existing_entries = set()
        gitignore_path = '.gitignore'
        
        try:
            if os.path.exists(gitignore_path):
                with open(gitignore_path, 'r') as f:
                    existing_entries = set(line.strip() for line in f if line.strip() and not line.startswith('#'))
        except Exception as e:
            logger.warning(f"Could not read existing .gitignore: {e}")
        
        # Find new entries to add
        new_entries = []
        for entry in gitignore_entries:
            if entry not in existing_entries:
                new_entries.append(entry)
        
        # Append new entries if any
        if new_entries:
            try:
                with open(gitignore_path, 'a') as f:
                    # Add header if file is new or empty
                    if not existing_entries:
                        f.write("# Polygon Data Extractor - Auto-generated entries\n")
                        f.write("# Generated by polygon_extractor.py\n\n")
                    else:
                        f.write("\n# Polygon Data Extractor files\n")
                    
                    for entry in new_entries:
                        f.write(f"{entry}\n")
                
                logger.info(f"✅ Added {len(new_entries)} entries to .gitignore")
                
                # Also create .gitignore in case it doesn't exist
                if not os.path.exists(gitignore_path):
                    logger.info("Created new .gitignore file")
                    
            except Exception as e:
                logger.warning(f"Could not update .gitignore: {e}")
        else:
            logger.info("✅ .gitignore already up to date")
    
    def init_database(self):
        """Create all necessary tables"""
        logger.info("Initializing SQLite database...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Enable foreign keys
        cursor.execute("PRAGMA foreign_keys = ON")
        
        # Tickers table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS tickers (
            ticker TEXT PRIMARY KEY,
            name TEXT,
            market TEXT,
            locale TEXT,
            primary_exchange TEXT,
            type TEXT,
            active BOOLEAN,
            currency_name TEXT,
            cik TEXT,
            composite_figi TEXT,
            share_class_figi TEXT,
            last_updated_utc TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # Snapshots table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS snapshots (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT,
            timestamp INTEGER,
            open REAL,
            high REAL,
            low REAL,
            close REAL,
            volume INTEGER,
            vwap REAL,
            prev_open REAL,
            prev_high REAL,
            prev_low REAL,
            prev_close REAL,
            prev_volume INTEGER,
            prev_vwap REAL,
            change REAL,
            change_percent REAL,
            last_trade_price REAL,
            last_trade_size INTEGER,
            last_trade_conditions TEXT,
            last_quote_bid REAL,
            last_quote_ask REAL,
            last_quote_bid_size INTEGER,
            last_quote_ask_size INTEGER,
            updated_at INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(ticker, timestamp)
        )
        """)
        
        # News table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS news (
            id TEXT PRIMARY KEY,
            publisher_name TEXT,
            publisher_homepage TEXT,
            publisher_logo TEXT,
            publisher_favicon TEXT,
            title TEXT,
            author TEXT,
            published_utc TEXT,
            article_url TEXT,
            tickers TEXT,  -- JSON array
            image_url TEXT,
            description TEXT,
            keywords TEXT,  -- JSON array
            amp_url TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # News tickers junction table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS news_tickers (
            news_id TEXT,
            ticker TEXT,
            PRIMARY KEY (news_id, ticker),
            FOREIGN KEY (news_id) REFERENCES news(id),
            FOREIGN KEY (ticker) REFERENCES tickers(ticker)
        )
        """)
        
        # Trades table (for demonstration - limited by API)
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS trades (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT,
            timestamp INTEGER,
            participant_timestamp INTEGER,
            price REAL,
            size INTEGER,
            conditions TEXT,  -- JSON array
            exchange INTEGER,
            trf_id INTEGER,
            sequence_number INTEGER,
            sip_timestamp INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(ticker, sip_timestamp, sequence_number)
        )
        """)
        
        # API call log
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS api_calls (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            endpoint TEXT,
            params TEXT,
            records_fetched INTEGER,
            response_time REAL,
            status_code INTEGER,
            error_message TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # Create indexes
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_snapshots_ticker ON snapshots(ticker)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_snapshots_timestamp ON snapshots(timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_news_published ON news(published_utc)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_trades_ticker ON trades(ticker)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_api_calls_endpoint ON api_calls(endpoint)")
        
        conn.commit()
        conn.close()
        logger.info("Database initialized successfully")
    
    def log_api_call(self, endpoint: str, params: Dict, records: int, response_time: float, 
                     status_code: int, error: Optional[str] = None):
        """Log API call details"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        INSERT INTO api_calls (endpoint, params, records_fetched, response_time, status_code, error_message)
        VALUES (?, ?, ?, ?, ?, ?)
        """, (endpoint, json.dumps(params), records, response_time, status_code, error))
        
        conn.commit()
        conn.close()
    
    def make_paginated_request(self, endpoint: str, params: Dict) -> List[Dict]:
        """Make paginated API requests and return all results"""
        all_results = []
        next_url = None
        page = 0
        
        while True:
            page += 1
            start_time = time.time()
            
            try:
                if next_url:
                    # Use the next_url directly
                    response = self.session.get(next_url)
                    url_for_log = next_url
                else:
                    # First request
                    url = f"{self.base_url}{endpoint}"
                    response = self.session.get(url, params=params)
                    url_for_log = url
                
                response_time = time.time() - start_time
                self.stats['api_calls'] += 1
                
                if response.status_code == 200:
                    data = response.json()
                    results = data.get('results', [])
                    all_results.extend(results)
                    
                    logger.info(f"Page {page}: Fetched {len(results)} records from {endpoint}")
                    self.log_api_call(endpoint, params, len(results), response_time, response.status_code)
                    
                    # Check for next page
                    next_url = data.get('next_url')
                    if not next_url or len(results) == 0:
                        break
                    
                    # Rate limiting - be respectful
                    time.sleep(0.2)
                else:
                    error_msg = f"API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    self.log_api_call(endpoint, params, 0, response_time, response.status_code, error_msg)
                    self.stats['errors'] += 1
                    break
                    
            except Exception as e:
                error_msg = f"Request error: {str(e)}"
                logger.error(error_msg)
                self.log_api_call(endpoint, params, 0, time.time() - start_time, 0, error_msg)
                self.stats['errors'] += 1
                break
        
        logger.info(f"Total records fetched from {endpoint}: {len(all_results)}")
        return all_results
    
    def extract_all_tickers(self, market: str = 'stocks', ticker_type: Optional[str] = None):
        """Extract all tickers with pagination"""
        logger.info(f"Extracting all {market} tickers...")
        
        params = {
            'market': market,
            'active': 'true',
            'limit': 1000,  # Max limit
            'order': 'asc',
            'sort': 'ticker',
            'apiKey': self.api_key
        }
        
        if ticker_type:
            params['type'] = ticker_type
        
        tickers = self.make_paginated_request('/v3/reference/tickers', params)
        
        # Save to database
        if tickers:
            self._save_tickers(tickers)
            self.stats['total_records'] += len(tickers)
        
        return tickers
    
    def _save_tickers(self, tickers: List[Dict]):
        """Save tickers to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for ticker in tickers:
            cursor.execute("""
            INSERT OR REPLACE INTO tickers 
            (ticker, name, market, locale, primary_exchange, type, active, 
             currency_name, cik, composite_figi, share_class_figi, last_updated_utc)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                ticker.get('ticker'),
                ticker.get('name'),
                ticker.get('market'),
                ticker.get('locale'),
                ticker.get('primary_exchange'),
                ticker.get('type'),
                ticker.get('active'),
                ticker.get('currency_name'),
                ticker.get('cik'),
                ticker.get('composite_figi'),
                ticker.get('share_class_figi'),
                ticker.get('last_updated_utc')
            ))
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {len(tickers)} tickers to database")
    
    def extract_all_snapshots(self, include_otc: bool = False):
        """Extract full market snapshot"""
        logger.info("Extracting market snapshots...")
        
        params = {
            'include_otc': str(include_otc).lower(),
            'apiKey': self.api_key
        }
        
        # This endpoint returns all tickers at once (no pagination needed)
        url = f"{self.base_url}/v2/snapshot/locale/us/markets/stocks/tickers"
        
        start_time = time.time()
        try:
            response = self.session.get(url, params=params)
            response_time = time.time() - start_time
            self.stats['api_calls'] += 1
            
            if response.status_code == 200:
                data = response.json()
                tickers_data = data.get('tickers', [])
                
                logger.info(f"Fetched {len(tickers_data)} snapshots")
                self.log_api_call('/v2/snapshot/locale/us/markets/stocks/tickers', 
                                params, len(tickers_data), response_time, response.status_code)
                
                if tickers_data:
                    self._save_snapshots(tickers_data)
                    self.stats['total_records'] += len(tickers_data)
                
                return tickers_data
            else:
                error_msg = f"Snapshot API error: {response.status_code}"
                logger.error(error_msg)
                self.log_api_call('/v2/snapshot/locale/us/markets/stocks/tickers', 
                                params, 0, response_time, response.status_code, error_msg)
                self.stats['errors'] += 1
                return []
                
        except Exception as e:
            error_msg = f"Snapshot request error: {str(e)}"
            logger.error(error_msg)
            self.stats['errors'] += 1
            return []
    
    def _save_snapshots(self, snapshots: List[Dict]):
        """Save snapshots to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for snap in snapshots:
            # Extract nested data
            day = snap.get('day', {})
            prev_day = snap.get('prevDay', {})
            last_trade = snap.get('lastTrade', {})
            last_quote = snap.get('lastQuote', {})
            
            cursor.execute("""
            INSERT OR REPLACE INTO snapshots
            (ticker, timestamp, open, high, low, close, volume, vwap,
             prev_open, prev_high, prev_low, prev_close, prev_volume, prev_vwap,
             change, change_percent, last_trade_price, last_trade_size,
             last_quote_bid, last_quote_ask, last_quote_bid_size, last_quote_ask_size,
             updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                snap.get('ticker'),
                int(time.time() * 1000),  # Current timestamp in ms
                day.get('o'), day.get('h'), day.get('l'), day.get('c'),
                day.get('v'), day.get('vw'),
                prev_day.get('o'), prev_day.get('h'), prev_day.get('l'), prev_day.get('c'),
                prev_day.get('v'), prev_day.get('vw'),
                snap.get('todaysChange'), snap.get('todaysChangePerc'),
                last_trade.get('p') if last_trade else None,
                last_trade.get('s') if last_trade else None,
                last_quote.get('p') if last_quote else None,
                last_quote.get('P') if last_quote else None,
                last_quote.get('s') if last_quote else None,
                last_quote.get('S') if last_quote else None,
                snap.get('updated')
            ))
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {len(snapshots)} snapshots to database")
    
    def extract_all_news(self, limit_per_page: int = 1000, max_pages: int = 10):
        """Extract news articles with pagination"""
        logger.info("Extracting news articles...")
        
        params = {
            'limit': limit_per_page,
            'order': 'desc',
            'sort': 'published_utc',
            'apiKey': self.api_key
        }
        
        # We'll limit pages to avoid pulling years of news
        all_news = []
        page_count = 0
        
        news = self.make_paginated_request('/v2/reference/news', params)
        
        if news:
            self._save_news(news)
            self.stats['total_records'] += len(news)
        
        return news
    
    def _save_news(self, news_items: List[Dict]):
        """Save news to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for article in news_items:
            publisher = article.get('publisher', {})
            
            # Save main article
            cursor.execute("""
            INSERT OR IGNORE INTO news
            (id, publisher_name, publisher_homepage, publisher_logo, publisher_favicon,
             title, author, published_utc, article_url, tickers, image_url,
             description, keywords, amp_url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article.get('id'),
                publisher.get('name'),
                publisher.get('homepage_url'),
                publisher.get('logo_url'),
                publisher.get('favicon_url'),
                article.get('title'),
                article.get('author'),
                article.get('published_utc'),
                article.get('article_url'),
                json.dumps(article.get('tickers', [])),
                article.get('image_url'),
                article.get('description'),
                json.dumps(article.get('keywords', [])),
                article.get('amp_url')
            ))
            
            # Save ticker relationships
            for ticker in article.get('tickers', []):
                cursor.execute("""
                INSERT OR IGNORE INTO news_tickers (news_id, ticker)
                VALUES (?, ?)
                """, (article.get('id'), ticker))
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {len(news_items)} news articles to database")
    
    def extract_sample_trades(self, ticker: str = 'AAPL', date: Optional[str] = None):
        """Extract sample trades for a ticker (limited by API)"""
        logger.info(f"Extracting sample trades for {ticker}...")
        
        if not date:
            date = datetime.now().strftime('%Y-%m-%d')
        
        params = {
            'timestamp': date,
            'limit': 50000,  # Max limit
            'apiKey': self.api_key
        }
        
        trades = self.make_paginated_request(f'/v3/trades/{ticker}', params)
        
        if trades:
            self._save_trades(trades, ticker)
            self.stats['total_records'] += len(trades)
        
        return trades
    
    def _save_trades(self, trades: List[Dict], ticker: str):
        """Save trades to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for trade in trades:
            cursor.execute("""
            INSERT OR IGNORE INTO trades
            (ticker, timestamp, participant_timestamp, price, size, conditions,
             exchange, trf_id, sequence_number, sip_timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                ticker,
                trade.get('timestamp'),
                trade.get('participant_timestamp'),
                trade.get('price'),
                trade.get('size'),
                json.dumps(trade.get('conditions', [])),
                trade.get('exchange'),
                trade.get('trf_id'),
                trade.get('sequence_number'),
                trade.get('sip_timestamp')
            ))
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {len(trades)} trades to database")
    
    def run_complete_extraction(self):
        """Run the complete extraction process"""
        self.stats['start_time'] = time.time()
        
        logger.info("=" * 60)
        logger.info("Starting Polygon.io Complete Data Extraction")
        logger.info("=" * 60)
        
        # 1. Extract all tickers
        logger.info("\n📊 PHASE 1: Extracting Tickers")
        tickers = self.extract_all_tickers(market='stocks')
        logger.info(f"✅ Extracted {len(tickers)} stock tickers")
        
        # Also get indices, ETFs if available
        indices = self.extract_all_tickers(market='indices')
        logger.info(f"✅ Extracted {len(indices)} index tickers")
        
        # 2. Extract market snapshots
        logger.info("\n📸 PHASE 2: Extracting Market Snapshots")
        snapshots = self.extract_all_snapshots(include_otc=False)
        logger.info(f"✅ Extracted {len(snapshots)} market snapshots")
        
        # 3. Extract news
        logger.info("\n📰 PHASE 3: Extracting News Articles")
        news = self.extract_all_news(limit_per_page=1000, max_pages=5)
        logger.info(f"✅ Extracted {len(news)} news articles")
        
        # 4. Extract sample trades for top tickers
        logger.info("\n💹 PHASE 4: Extracting Sample Trades")
        top_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']
        for ticker in top_tickers:
            trades = self.extract_sample_trades(ticker)
            logger.info(f"✅ Extracted {len(trades)} trades for {ticker}")
            time.sleep(1)  # Rate limiting
        
        # Calculate final statistics
        elapsed_time = time.time() - self.stats['start_time']
        
        logger.info("\n" + "=" * 60)
        logger.info("EXTRACTION COMPLETE - SUMMARY")
        logger.info("=" * 60)
        logger.info(f"Total API Calls: {self.stats['api_calls']}")
        logger.info(f"Total Records Extracted: {self.stats['total_records']:,}")
        logger.info(f"Total Errors: {self.stats['errors']}")
        logger.info(f"Total Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)")
        logger.info(f"Average Time per API Call: {elapsed_time/self.stats['api_calls']:.2f} seconds")
        logger.info(f"Database Size: {os.path.getsize(self.db_path) / 1024 / 1024:.2f} MB")
        
        # Generate database summary
        self._generate_db_summary()
    
    def _generate_db_summary(self):
        """Generate summary of data in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        logger.info("\n📊 DATABASE SUMMARY:")
        
        tables = ['tickers', 'snapshots', 'news', 'trades', 'api_calls']
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            logger.info(f"  {table}: {count:,} records")
        
        # Additional statistics
        cursor.execute("SELECT COUNT(DISTINCT ticker) FROM snapshots")
        unique_tickers = cursor.fetchone()[0]
        logger.info(f"\n  Unique tickers with snapshots: {unique_tickers:,}")
        
        cursor.execute("SELECT MIN(published_utc), MAX(published_utc) FROM news")
        date_range = cursor.fetchone()
        if date_range[0]:
            logger.info(f"  News date range: {date_range[0]} to {date_range[1]}")
        
        conn.close()


def main():
    """Main function"""
    # Get API key
    API_KEY = os.getenv('API_KEY')
    if not API_KEY:
        logger.error("❌ API_KEY not found in .env file!")
        sys.exit(1)
    
    # Create extractor
    extractor = PolygonDataExtractor(API_KEY)
    
    # Run complete extraction
    try:
        extractor.run_complete_extraction()
        
        logger.info("\n✅ All data successfully extracted and stored in polygon_complete_data.db")
        logger.info("You can query the database using any SQLite client.")
        
    except KeyboardInterrupt:
        logger.info("\n⚠️ Extraction interrupted by user")
    except Exception as e:
        logger.error(f"\n❌ Extraction failed: {str(e)}", exc_info=True)


if __name__ == "__main__":
    main()